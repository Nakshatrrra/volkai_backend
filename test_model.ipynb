{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f333823-33da-4680-bd6c-e5f60fe70af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral_120k_20feb\n",
    "# nakshatra44/mistral_21_2epoches_90k_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51807b58-7948-4b68-b348-3a3be4ff5349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.5: Fast Mistral patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "### Context: \n",
      "\n",
      "### Human: Who are you \n",
      "\n",
      "### Assistant:\n",
      "\n",
      "Generated Response:\n",
      "Main Volkai hoon, ek AI assistant jo Kairosoft AI Solutions Limited ne develop kiya hai. Top 50 BSE companies mein ek vishwasniya naam ke roop mein, main aapke sawalon ka jawab dene ke liye yahan hoon.<|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "# Define settings (should match your training settings)\n",
    "max_seq_length = 2048\n",
    "dtype = None            # None for auto-detection; you may also specify torch.float16 or bfloat16\n",
    "load_in_4bit = True     # Use 4-bit quantization if that was used during training\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load your incrementally fine-tuned model and tokenizer\n",
    "# -------------------------------\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"mistral_21_2epoches_90k_v3\",   # Folder where your incrementally trained model is saved\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# IMPORTANT: Prepare the model for inference!\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Prepare a prompt for testing\n",
    "# -------------------------------\n",
    "prompt = \"### Context: \\n\\n### Human: Who are you \\n\\n### Assistant:\"\n",
    "\n",
    "# Tokenize the prompt and move to the appropriate device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Stream a response from the model\n",
    "# -------------------------------\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# Generate tokens in a separate thread\n",
    "thread = Thread(target=model.generate, kwargs={\"input_ids\": inputs[\"input_ids\"], \"streamer\": streamer, \"max_new_tokens\": 100, \"do_sample\": True, \"temperature\": 0.5, \"top_p\": 0.8})\n",
    "thread.start()\n",
    "\n",
    "# Stream response word-by-word\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\nGenerated Response:\")\n",
    "for text in streamer:\n",
    "    print(text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acc566-46a3-4112-ba22-dad79bdda020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "###Context: \n",
      "\n",
      "###Human: Can you help me creating study time table? \n",
      "\n",
      "### Assistant:\n",
      "\n",
      "Generated Response:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 2. Prepare a prompt for testing\n",
    "# -------------------------------\n",
    "prompt = \"###Context: \\n\\n###Human: Can you help me creating study time table? \\n\\n### Assistant:\"\n",
    "\n",
    "# Tokenize the prompt and move to the appropriate device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Stream a response from the model\n",
    "# -------------------------------\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# Generate tokens in a separate thread\n",
    "thread = Thread(target=model.generate, kwargs={\"input_ids\": inputs[\"input_ids\"], \"streamer\": streamer, \"max_new_tokens\": 100, \"do_sample\": True, \"temperature\": 0.5, \"top_p\": 0.8})\n",
    "thread.start()\n",
    "\n",
    "# Stream response word-by-word\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\nGenerated Response:\")\n",
    "for text in streamer:\n",
    "    print(text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3becc8-071a-4b96-ae19-cb4ca4a3b747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
